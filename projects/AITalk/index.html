<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DCPTalk</title>
  <link rel="icon" href="github-fill.png" sizes="16x16"><!-- 图标-->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            AITalk: Alleviating Imbalances in Speech-Driven 3D Facial Animation
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block" >Zhaojie Chu<sup>1</sup>,</span>
            <span class="author-block" >Kailing Guo<sup>1</sup>,</span>
            <span class="author-block" >Xiaofen Xing<sup>1</sup>,</span>
            <span class="author-block" >Pengsheng Liu<sup>1</sup>,</span>
            <span class="author-block" >Bolun Cai<sup>2</sup>,</span>
            <span class="author-block" >Xiangmin Xu<sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>South China University of Technology,</span>
            <span class="author-block"><sup>2</sup>ByteDance Inc</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#teaser"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zjchu/AITalk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>

  </div>


</section class="hero teaser">
      <!-- Paper video.--> 
  <center>
    <video id="teaser" autoplay loop controls playsinline height="900" width="900">
        <source src="./static/videos/demo.mp4" type="video/mp4">
    </video></center>
  <h2 class="subtitle has-text-centered">
    <strong>DCPTalk</strong> can synthesize realistic 3D facial animations (mesh sequences) given audio snippets.
  </h2>
     <!--Paper video. -->


<section class="section" id="Facial Activity Intensity">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven 3D facial animation has emerged as a hot topic; during this process, movements across facial regions 
            are interdependent, they are influenced by the intricate interactions among facial muscles, and they manifest 
            personalized differences. The existing methods typically simplify the facial animation generation task to an 
            infinitely thin surface skin deformation without an underlying structure, thereby ignoring the intricate and 
            personalized dynamics of facial muscle activity. These methods tend to produce static or weak upper-face animations 
            with an average facial movement style.
          </p>
          <p>
            In this work, we propose a novel framework, DCPTalk, to mimic the intricate dynamics of facial muscle activity and portray 
            personalized facial animations. Based on facial dynamic coupling properties, we propose Mouth2Face to simulate the facial 
            muscle control system, yielding realistic and coordinated facial animations evoked by mouth movements. Mouth movements are 
            easily synthesized from speech signals due to their direct correlation with phonetic articulation and vocal tract dynamics. 
            To further enhance the detail of facial movements, we employ surface skin deformation to refine the facial animation derived 
            from Mouth2Face. Furthermore, personal factors, including inherent physical traits and acquired speaking styles, directly determine 
            the uniqueness and realism of facial animations. Inherent physical traits are embedded into Mouth2Face for constructing personalized 
            facial muscle control system, while acquired speaking styles are employed to modulate external driving signals.
          </p>
          <p>
            Extensive qualitative and quantitative experiments as well as a user study indicate that DCPTalk outperforms the existing 
            state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    
    <h2 class="title">Method</h2>
    <center><div><img src="./static/images/Framework.png" ></center>
    <p class="content has-text-justified">
      Overview of the proposed DCPTalk. The novel framework mimics the intricate dynamics of facial muscle 
      activity and portrays personalized facial animations. The heart of DCPTalk is Mouth2Face, which utilizes 
      mouth movements \(\mathbf{\hat{M}}\) to evoke facial animations \(\mathbf{\hat{Y}}_M\). Mouth movements 
      are easily synthesized from speech signals due to their direct correlation with phonetic articulation and 
      vocal tract dynamics. The \(\mathbf{\hat{Y}}_M\) are refined by using \(\mathbf{F}_P\) to further enhance 
      the facial movement details. Facial animations are also influenced by personal factors \(\mathbf{S}\), 
      including inherent physical traits and acquired speaking styles. The inherent physical traits are integrated 
      into Mouth2Face to construct a personalized facial muscle control system, while the acquired speaking styles 
      are utilized to modulate pseudo landmarks \(\mathbf{L}\) and speech features \(\mathbf{F}_A\).
    </p>
    
  </div>
  
</section>

<section class="section" id="Comparison">
  <div class="container is-max-desktop content">
    <h2 class="title">Comparison</h2>
    <center><div><img src="./static/images/quantitative1.png" ></center>
    <p class="content has-text-justified">
      Visual comparison among the facial animations generated by various methods across the BIWI-Test-B (top), the Multiface-Test-B (middle), and the
      VOCA-Test (bottom) datasets. Our proposed method exhibits more precise speech-lip synchronization.
    </p>
    
  </div>

  <div class="container is-max-desktop content">
    <h2 class="title">Comparison</h2>
    <center><div><img src="./static/images/quantitative2.png" ></center>
    <p class="content has-text-justified">
      Visualization of the temporal statistics of adjacent-frame motion variations across BIWI-Test-B (top), Multiface-Test-B (middle), and VOCA-Test
      (bottom) datasets. Inter-frame motion L2 distance is employed to obtain the mean and standard deviation (Std) at each vertex, with higher mean indicating
      stronger facial movements and higher standard deviation reflecting richer variations in facial dynamic.
    </p>
    
  </div>
  
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> 
            project page.

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
