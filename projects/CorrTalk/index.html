<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CorrTalk</title>
  <link rel="icon" href="github-fill.png" sizes="16x16"><!-- 图标-->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block" >Zhaojie Chu<sup>1</sup>,</span>
            <span class="author-block" >Kailing Guo<sup>1</sup>,</span>
            <span class="author-block" >Xiaofen Xing<sup>1</sup>,</span>
            <span class="author-block" >Yilin Lan<sup>1</sup>,</span>
            <span class="author-block" >Bolun Cai<sup>2</sup>,</span>
            <span class="author-block" >Xiangmin Xu<sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>South China University of Technology,</span>
            <span class="author-block"><sup>2</sup>ByteDance Inc</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#teaser"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zjchu/CorrTalk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
      <!-- Paper video.  -->
    <!-- <div class="columns is-centered has-text-centered">   
        <div class="publication-video">
          <center><video width="480" height="480" controls><source src="May1.mp4" type="video/mp4"></video></center>
        </div>
    </div> -->
     <!-- Paper video. -->
  </div>


</section class="hero teaser">
      <!-- Paper video.--> 
  <center>
    <video id="teaser" autoplay loop controls playsinline height="900" width="900">
        <source src="./static/videos/demo.mp4" type="video/mp4">
    </video></center>
  <h2 class="subtitle has-text-centered">
    <strong>CorrTalk</strong> can synthesize vivid 3D facial animations (mesh sequences) given audio snippets.
  </h2>
     <!--Paper video. -->


<section class="section" id="Facial Activity Intensity">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing 
            research interest. During speaking activities, the mouth displays strong motions, 
            while the other facial regions typically demonstrate comparatively weak activity levels. 
            Existing approaches often simplify the process by directly mapping single-level speech features 
            to the entire facial animation, which overlook the differences in facial activity intensity leading 
            to overly smoothed facial movements.
          </p>
          <p>
            In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal 
            correlation between hierarchical speech features and facial activities of different intensities across 
            distinct regions. A novel facial activity intensity metric is defined to distinguish between strong and 
            weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements. 
            Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize 
            strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, 
            a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical 
            speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions.
          </p>
          <p>
            Extensive qualitatively and quantitatively experiments as well as a user study indicate that our 
            CorrTalk outperforms existing state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    
    <h2 class="title">Facial Activity Intensity</h2>
    <center><div><img src="./static/images/1.png" width="500" height="400" style="border-right:2px solid #ccc" >
    <img src="./static/images/1.gif" width="280" height="400" ></center>
   

    <p>
      CorrTalk first analyses differences in facial activity intensity cross distinct regions. 
      Facial activity intensity is quantified using amplitude values within the fundamental band of the short-time Fourier transform(STFT).
      <b > Left: </b> activity intesity of a vertex in mouth and forehead region within a motion sequence are shown in (a) and (c) 
      (top row: \(L_{2}\) distance between vertices in the reference sequence and the neutral topology; 
      bottom row:  STFT of the vertex displacements.). 
      (b) represents the average facial activity intensity from the training data. 
      <b> Right: </b> dynamics of facial activity intensity in a sequence. 
    </p>


    <h2 class="title">Method</h2>
    <center><div><img src="./static/images/2.png" ></center>
    <p>
      Overview of the proposed CorrTalk. A novel framework for learning the temporal correlation between HSF and 
      facial activities of different intensities uses raw audio as input and generates a sequence of 3D facial animation. 
      The design of the acoustic feature extractor follows wavLM. The weighted hierarchical speech encoder produces frame-, 
      phoneme-, word- and utterance-level speech features, and calculates the importance weight of each level of features 
      for strong and weak facial movements. A dual-branch decoder based on the FAI synchronously generates strong and weak 
      facial movements. After performing STFT of the vertex displacements from training data, a learnable 
      mask \(\mathbf{m}_t \in [0, 1] \) is initialized according to the absolute value of the amplitude in 
      fundamental frequency. \(\mathbf{m}_t (\cdot) \) close to 1 indicates strong facial movements and vice 
      versa for weak movements.
    </p>
    
  </div>
  
</section>

<section class="section" id="Comparison">
  <div class="container is-max-desktop content">
    <h2 class="title">Comparison</h2>
    <center><div><img src="./static/images/3.png" ></center>
    <p>
      Visual comparison of sampled facial animations generated by different methods on VOCA-Test (left) and 
      BIWI-Test-B (right). The top portion delineates facial animations associated with distinct speech content. 
      The bottom portion displays the synthetic sequence with ground truth mean error.
    </p>
    
  </div>
  
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> 
            project page.

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
