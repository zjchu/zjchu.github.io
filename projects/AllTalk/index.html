<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AllTalk</title>
  <link rel="icon" href="github-fill.png" sizes="16x16"><!-- 图标-->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            Alleviating One-to-many Mapping in Talking Head Synthesis with Dynamic Adaptation Context and Style Adapter
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block" >Zhaojie Chu<sup>1</sup>,</span>
            <span class="author-block" >Kailing Guo<sup>1</sup>,</span>
            <span class="author-block" >Xiaofen Xing<sup>1</sup>,</span>
            <span class="author-block" >Bolun Cai<sup>2</sup>,</span>
            <span class="author-block" >Shan He<sup>3</sup>,</span>
            <span class="author-block" >Xiangmin Xu<sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>South China University of Technology,</span>
            <span class="author-block"><sup>2</sup>ByteDance Inc,</span>
            <span class="author-block"><sup>3</sup>iFlytek Research.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/EsnLAclo3Vc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zjchu/AllTalk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>

  </div>


</section class="hero teaser">
      <!-- Paper video.--> 
  <center>
  <iframe id="teaser" width="900" height="450" src="https://www.youtube.com/embed/EsnLAclo3Vc" 
    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  </center>



<section class="section" id="Facial Activity Intensity">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven talking head synthesis technology has made remarkable progress, but it still faces the challenge of 
            the one-to-many pathological mapping. The challenge results in inaccurate lip movements, ambiguity in facial expressions, 
            and a lack of coherence during transitions between facial movements. The phenomenon is primarily caused by: (1) for one speaker, 
            the same phoneme corresponds to a wide range of mouth shapes and facial expressions due to contextual variations, and (2) 
            for the same spoken content, different speakers exhibit diverse facial expressions as a result of unique speaking styles. 
          </p>
          <p>
            In this work, we propose a novel framework, called AllTalk, to alleviate one-to-many pathological mapping, which enables more 
            vivid and natural talking head. Specifically, considering the asymmetry and dynamic nature of mouth shapes' dependence on 
            phoneme context, we propose a Dynamic Adaptive Context encoder to accurately capture the context around the phoneme and 
            its dynamics, thereby reducing the ambiguity in mapping speech to facial movements. Moreover, to alleviate the uncertainty 
            caused by individual stylistic differences, we propose a Style Adapter that expands a generic discrete motion space for 
            target speaker. The Style Adapter not only effectively represents general facial motions but also captures the personalized 
            nuances of facial movements. To further enhance the fidelity of output, we introduce a Dynamic Gaussian Renderer based on 
            3D Gaussian Splatting, capable of producing stable and realistic rendering videos.
          </p>
          <p>
            Extensive qualitative and quantitative experiments demonstrate that AllTalk surpasses existing state-of-the-art methods, 
            providing an effective solution to the challenge of one-to-many mapping.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    
    <h2 class="title">Examples of One-to-many Mapping</h2>
    <center><div>
      <!-- <img src="./static/images/Face_muscle.jpg" width="500" height="400" style="border-right:2px solid #ccc" > -->
    <img src="./static/images/one-to-many-a.png" >
    </center>
  
    <p class="content has-text-justified">
      (a) For one speaker, one phoneme corresponds to multiple alternative mouth shapes due to complex contexts.
    </p>


    <center><div>
      <!-- <img src="./static/images/Face_muscle.jpg" width="500" height="400" style="border-right:2px solid #ccc" > -->
    <img src="./static/images/one-to-many-b.png" >
    </center>

    <p class="content has-text-justified">
      (b) For the same phoneme whithin same context, different speakers exhibit distinct mouth shapes, reflecting their unique speaking styles.
    </p>


</section> -->

<section class="section" id="Comparison">
  <div class="container is-max-desktop content">
    <h2 class="title">Comparison</h2>
    <center><div><img src="./static/images/result1.png" ></center>
    <p class="content has-text-justified">
      Comparative visualization of facial movements synthesized by different models for the same phoneme in difference 
      contexts. “/æ/” exhibits distinct mouth shapes when pronouncing the words “sadness” and “and”, and “/m/” displays different
      facial movements when articulating the words “tomorrow” and “meet”.
    </p>

    <center><div><img src="./static/images/result2.png" ></center>
      <p class="content has-text-justified">
        Comparative visualization of facial movements synthesized by different models for the same spoken content cross
        different speakers. For the same word, different speakers displayed diverse mouth shapes.
      </p>
    
  </div>
 
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> 
            project page.

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
